2021/5/4
- TODO: new sample point selection
  - Probability Density Function, from wikipedia:
    a probability density function (PDF), or density of a continuous random variable,
    is a function whose value at any given sample (or point) in the sample space
    can be interpreted as providing a relative likelihood that the value of
    the random variable would equal that sample
  - So basically the higher the intensity, the more likely it is for a random point
    to equal that intensity?
- DONE: confirm that the interpolation function is working / able to draw
  - think there's an issue with sample point selection
    - currently just generating random coordinates and saving them if the intensity
      is above a defined threshold
    - paper: "the ref image intensity is used as a probability density function"
      - this means that the intensities of the pixel values are essentially made into
        a histogram / bell curve type of thing
    - paper: generating sample points stops when |X_sample| = (1/17) |X|
      - thought that this meant the NUMBER of sample points = 1/17 the number of pixels in the image
      - it might actually mean that the TOTAL INTENSITY of the sample points = 1/17 total intensity of image

2021/5/2
- turn interpolation image from clone -> make entirely new image

2021/4/29
meeting
  - random points: HSFC number line vs random x and random y?
  - for-each loop bug
  - making custom histogram for interpolation
    - number of histogram bins?
  - testing with low sample points -> no good
    - number of sample points?
  - scaling xHat output to range of 0.0-1.0?

to do
  - optimization
    - find which processes are taking the most time
      - TODO: use chrono to measure time taken for:
        - OMP estimation of each point?
        - OMP estimation of all points
        - gathering sample points
        - interpolatiion of all points
        https://www.geeksforgeeks.org/chrono-in-c/
    - TODO: parallelize OMP estimation
      - https://www.geeksforgeeks.org/multithreading-in-cpp/
      - probably divide up the sample_points array into parts
        for each thread to handle
      - double check params and stuff (adding const when necessary)
        to make sure nothing is getting changed / everything is concise
  - TODO: make a DEBUG constant to enable / disable debug output
  - interpolation
    - TODO: make interpolation squares be drawn on a separate image
      - how to parallelize this?
  - TODO: improve sampling to be a proper PDF
  - TODO:
    -


2021/4/27
- interpolation
  - opencv calcHist() just outputs a 1D array with the number of pixels in each bin
    - NOTE: output doesn't contain the actual pixel values of each pixel in each bin
    - need to figure out a different approach
- clean up code
- Q: looking at OMP coarse DM stuff, "only the displacement d(ξ) corresponding to
the coefficient with the highest magnitude is kept for our further
processing"
  - say xHat has -1.2 and 1.1 in it. -1.2 has highest magnitude -> do I return -1.2 or 1.2?
- NOTE: img height: 338 with 41 px border -> actually 256
        img width : 784 with 41 px border -> actually 702
        179712 pixels -> 10571 sample points? (1/17th)
        img.cols -> x, img.rows -> y



2021/4/25
- DONE change how OMP translates to image
  - now want to have the highest absolute value from output
    set to pixel value of what was passed in
    - still have the pass by reference stuff get changed,
      OR just have the OMP function return that highest value?
    - returning just highest value would be way simpler -> do that

- interpolation
  - for each pixel chosen for OMP estimation, create a 13 x 13 box around it
  - make a histogram based on this box. choose bin with highest number.
    - how many bins?
    - DONE: change chosen OMP coords to a vector you push_back into
      - allows you to switch easily between grid of coordinates
        and what they actually use in the paper
  - set all pixels in output image within the 13 x 13 box to the average value of the chosen bin
    - color scaling?

- DONE coordinate selection
  - ref image intensity used as probability density function to generate sample points
    - basically you just want to choose random points that have a high intensity
    - choose random point -> keep if it's intensity is above threshold
      - DONE: make histogram of ref image intensities to determine appropriate threshold
        - basic distribution of point intensities:
          between 0 and 0.25:   0
          between 0.25 and 0.5: 230905
          between 0.5 and 0.75: 31244
          between 0.75 and 1.0: 2843
          total of 264992 points
        - chosen threshold of 0.75?
        - num chosen points = 1/17 * num pixels in ref image?
          - would be 15588 points in our case -> lower the threshold?
  - 2 references listed here
    - C. Schretter and H. Niederreiter, “A direct inversion method for non-
      uniform quasi-random point sequences,” Monte Carlo Methods Appl.,
      vol. 19, no. 1, pp. 1–9, 2013
    - C.-C. Wu and Y.-I. Chang, “Approximately even partition algorithm for
      coding the Hilbert curve of arbitrary-sized image,” IET Image Process.,
      vol. 6, no. 6, pp. 746–755, 2012
  - NOTE: points are initially randomly chosen by picking random number along
          Hilbert Space-Filling curve! Instead of generating 2 random points?
  - TODO: add HSFC (for rectangular areas)? or just randomly generate x and y?



2021/4/22
- fix: converting original cartesian to float image didnt actually change the pixel values
  needed to include /= 255 to scale values
- fix: result of OMP estimation was saving weird values in the column dedicated to error vector
- fix xHat = 0 issue. to do so, look at xTemp, supports, dictASupport
	- xTemp has some negative values? not sure if this is normal
- read paper to develop a plan of action

2021/4/17
- pad sonar image with 41 black pixels
- set up collage of OMP outputs
- read research paper to figure out how many points to do OMP at

2021/4/3
- fixed search areas in getGamma and dictionaryMatrix
	- for loop needed to use <= instead of <
- NOTE: double check how to use coordinates in openCV (x, y) or (y, x)
- convert everything to floats if possible
- ISSUE:
	- getting strange values in targetY gamma
		- values are either very very small (close to 0), or huge
		- how does converting the opencv image effect scaling of grayscale values?
			- when i write "float" in c++, does it expect a 32bit float that is being used by openCV?
	- norm of error vector = NaN?
		- likely due to huge values, may be reaching limit of float datatype?
	- original image returned by mitchell's imageproc file is CV_8UC1
		- make sure it can convert correctly
	- bunch of values in error vector are getting set to NaN!
		- error = targetY - (dictA * xHat);
		- check that all values in targetY, dictA, xHat are not equal to NaN!
			- have a feeling that it may be due to not doing any padding on the image
			  dictionary matrix may be going off the image, but I assume that would throw
				an error rather than just set to NaN


2021/4/1
- code working better now
- issue updating xHat
	- "For every index in S (support vector), xHat = xTemp. otherwise xHat == 0"
	- so if column 700 was chosen in the first iteration (AKA support = [700, null, null, ...]),
	  and I am currently looping over support vector with variable j (j currently = 0), do I set
		xHat[700] = xTemp[700]? or:
		xHat[dictA.col(700)[0]] = xTemp[dictA.col(700)[0]]


2021/3/31
- stuck at turning error vector into a float datatype
	- when I do, the multiplication at line 114 stops compiling bc of mixing datatypes
- also realized i forgot to do the pseduo inverse of dictA for x_temp (as detailed in the OMP writeup)

2021/3/30
- NOTE: matrix notation is row, column
- issue of using Dynamic matrices in Eigen
	- for any multiplication, need to know that the dimensions of the matrices will work?
	- ! may need to change vectors to 1 x Dynamic matrices
		- Assertion `lhs.cols() == rhs.rows()
		- xHat cols: 1 xHat rows: 169 dictA rows: 169
- Eigen stack memory limit
	- trying to use big matrices -> errors
	- need to circumvent (i think need to change compilation settings? maybe cmake)
	-
For speeding up matrix multiplication:
https://stackoverflow.com/questions/39723461/vector-matrix-multiplication-with-eigen
1. try converting all vectors to 1 x Dynamic Matrices
2. Need to pass parameters by REFERENCE, not by value!
left off: having trouble around line 114 in coarseDM
can't get dictionary matrix (169 by 1681) to multiply with xHat (169 by 1)

Still can't figure out why it's broken... error message:
xHat cols: 1
xHat rows: 169
dictA cols: 1681
dictA rows: 169
sonar_listener: /usr/include/eigen3/Eigen/src/Core/Product.h:95: Eigen::Product<Lhs, Rhs, Option>::Product(const Lhs&, const Rhs&) [with _Lhs = Eigen::Matrix<int, -1, -1>; _Rhs = Eigen::Matrix<int, -1, -1>; int Option = 0; Eigen::Product<Lhs, Rhs, Option>::Lhs = Eigen::Matrix<int, -1, -1>; Eigen::Product<Lhs, Rhs, Option>::Rhs = Eigen::Matrix<int, -1, -1>]: Assertion `lhs.cols() == rhs.rows() && "invalid matrix product" && "if you wanted a coeff-wise or a dot product use the respective explicit functions"' failed.
Aborted (core dumped)




- order of hilbert curve = length or width of sqaure it is traversing
	- 8 x 8 square can have every coordinate filled by a N = 8 curve
	- how do you do a hilbert curve in a rectangle?

notes on OMP
- Ax = y
	- A = dictionary matrix
	- x = unknown vector (will be 1 x 169, a column of A)
	- y = target vector (1 x 169 gamma vector centered on matching pixel)
- approach: instead attempt to solve:
- y - Ax = e
	- e = error vector. if x exists (highly unlikely), e would = 0
	- instead we are looking for the best possible value of x, and
	  include e to make up for the fact that it isn't perfect
SETUP OMP algorithm
- output vector x^ (x-hat), our proposed solution to x
	- init as 0, iterate on it throughout algo
- error vector e
	- init as y (if x^ = 0, then e must = y)
- support vector S
	- init empty. it will eventually contain index of A we want for
	  our solution
BEGIN LOOP
	- loop ends once error is small enough (ex: norm of e > 0.1)
		- what does norm mean
- loop through A to find index j that is maximally correlated with e
	- at each index j, calculate:
		- abs((A_j) ^ transpose e) / norm(A_j)
		- "the norm of the matrix product between A_j transpose
		  and the current error, divided by euclidean norm of A_j"
	- the max output of the above calculation will be the vector we
	  append to S
		- by "append" it means add the specific column to S
		- ex: if first loop finds j = 2 is best, S = [2]
		  if second loop finds j = 5, S = [2,5]
	- now update x^
		- take Moore-Penrose pseudo-inverse of A
		  = (A_transpose A) ^ (inverse)A_transpose
			- available on most any lin alg library
		- multiply y by this value to get x_temp
			- x_temp = A_pseudo_inverse * y
(?)		- can now cast x_temp directly to x^
(?)			- for every index in S, set x_hat == x_temp
	- update error vector
		- e = y - A*x^
	- check norm(e) << threshold OR exceeded max iterations, exit if needed
